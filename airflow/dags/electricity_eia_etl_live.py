#!/usr/bin/env python3
"""
Electricity Demand ETL DAG - PRODUCTION READY (Professor's Pattern)
- Parallel API fetch (ThreadPoolExecutor) - 6 regions simultaneously
- Direct streaming to Snowflake temp table (NO XCom for large data)
- MERGE (UPSERT) with proper parameter binding
- BEGIN/COMMIT/ROLLBACK transactions everywhere
- Try/except/raise error handling
- Incremental load with backfill support
- Idempotent operations
- Batch commits every 5000 records
"""
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from datetime import datetime, timedelta
import requests
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed


logger = logging.getLogger(__name__)


# ========================
# DAG Configuration
# ========================
default_args = {
    'owner': 'data_eng',
    'start_date': datetime.now() - timedelta(days=1),  # Yesterday - prevents backfill
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(minutes=30),
}


dag = DAG(
    'electricity_eia_etl_live',
    default_args=default_args,
    description='ETL: EIA electricity data → Snowflake (streaming, no XCom)',
    schedule_interval='0 * * * *',  # HOURLY
    catchup=False,  # Disable backfill - only run from now forward
    max_active_runs=1,  # Process one at a time for safety
    tags=['ETL', 'EIA', 'Hourly', 'Production'],
)


DB = "USER_DB_HEDGEHOG"
SCHEMA = "ELECTRICITY_RAW"
TABLE = "EIA_HOURLY_DEMAND"
TEMP_TABLE = "EIA_HOURLY_DEMAND_TEMP"
BATCH_SIZE = 5000


# ========================
# Helper: Get Snowflake Connection
# ========================
def get_snowflake_cursor():
    """Get Snowflake cursor from Airflow Connection with error handling"""
    try:
        hook = SnowflakeHook(snowflake_conn_id="snowflake_conn")
        conn = hook.get_conn()
        return conn.cursor()
    except Exception as e:
        logger.error(f"Failed to connect to Snowflake: {e}")
        raise


def ensure_schema_exists():
    """
    Create schema and main table if they don't exist (idempotent)
    - Wrapped in BEGIN/COMMIT/ROLLBACK
    - Try/except/raise error handling
    """
    cur = get_snowflake_cursor()
    
    try:
        # BEGIN transaction
        logger.info("Ensuring schema and tables exist...")
        cur.execute("BEGIN")
        
        # Create schema
        cur.execute(f"CREATE SCHEMA IF NOT EXISTS {DB}.{SCHEMA}")
        logger.info(f"Schema {DB}.{SCHEMA} ready")
        
        # Create main table with proper structure
        cur.execute(f"""
        CREATE TABLE IF NOT EXISTS {DB}.{SCHEMA}.{TABLE} (
            PERIOD_DATE TIMESTAMP NOT NULL,
            RESPONDENT VARCHAR(50) NOT NULL,
            RESPONDENT_NAME VARCHAR(255),
            DATA_TYPE VARCHAR(20) NOT NULL,
            DATA_TYPE_NAME VARCHAR(255),
            VALUE FLOAT,
            VALUE_UNITS VARCHAR(50),
            LOADED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
            PRIMARY KEY (PERIOD_DATE, RESPONDENT, DATA_TYPE)
        )
        """)
        logger.info(f"Table {DB}.{SCHEMA}.{TABLE} ready")
        
        # COMMIT transaction
        cur.execute("COMMIT")
        logger.info("✅ Schema and table verified")
        
    except Exception as e:
        # ROLLBACK on error
        logger.error(f"❌ Schema setup error: {e}")
        try:
            cur.execute("ROLLBACK")
            logger.info("Transaction rolled back")
        except Exception as rollback_error:
            logger.error(f"Rollback failed: {rollback_error}")
        raise
    finally:
        cur.close()


# ========================
# Task 1: Fetch and Stream to Temp Table
# ========================
def fetch_region_data(region, api_key, days_back=7):
    """Fetch single region data (runs in parallel thread)"""
    base_url = "https://api.eia.gov/v2/electricity/rto/region-data/data"
    records = []
    
    logger.info(f"Fetching region: {region}")
    offset = 0
    page_size = 5000
    max_records = 50000  # Limit per region to prevent memory issues
    
    while len(records) < max_records:
        params = {
            "api_key": api_key,
            "frequency": "hourly",
            "data[0]": "value",
            "facets[respondent][]": region,
            "offset": offset,
            "length": page_size,
            "sort[0][column]": "period",
            "sort[0][direction]": "desc",
        }
        
        try:
            response = requests.get(base_url, params=params, timeout=30)
            
            if response.status_code != 200:
                logger.warning(f"{region}: HTTP {response.status_code}")
                break
            
            data = response.json()
            page_records = data.get("response", {}).get("data", [])
            
            if not page_records:
                break
            
            records.extend(page_records)
            logger.info(f"{region}: Fetched {len(page_records)} records (total: {len(records)})")
            
            # Check if we've reached the end
            total_available = int(data.get("response", {}).get("total", 0))
            if offset + page_size >= total_available:
                break
            
            offset += page_size
            
        except Exception as e:
            logger.error(f"{region} error: {e}")
            break
    
    logger.info(f"{region} complete: {len(records)} records")
    return records


def fetch_and_load_to_temp(**context):
    """
    Fetch data from EIA API in parallel and stream directly to Snowflake temp table.
    - NO XCom for large data (avoids memory overflow)
    - BEGIN/COMMIT/ROLLBACK transaction
    - Try/except/raise error handling
    - Idempotent: DROP and recreate temp table
    - Backfill support: Uses execution_date
    """
    try:
        api_key = Variable.get("eia_api_key")
    except Exception as e:
        logger.error(f"Failed to get EIA API key: {e}")
        raise
    
    regions = ["US48", "CISO", "TEXA", "MISO", "NYISO", "PJM"]
    execution_date = context.get('ds', 'manual')  # Get execution date for logging
    
    logger.info(f"Starting parallel fetch for execution_date: {execution_date}")
    
    # Ensure schema exists (with transaction handling)
    ensure_schema_exists()
    
    # Get Snowflake cursor
    cur = get_snowflake_cursor()
    
    try:
        # BEGIN transaction
        logger.info("Starting ETL transaction...")
        cur.execute("BEGIN")
        
        # Create temp table (idempotent - drop and recreate)
        logger.info("Creating temp table...")
        cur.execute(f"DROP TABLE IF EXISTS {DB}.{SCHEMA}.{TEMP_TABLE}")
        cur.execute(f"""
        CREATE TABLE {DB}.{SCHEMA}.{TEMP_TABLE} (
            PERIOD_DATE TIMESTAMP NOT NULL,
            RESPONDENT VARCHAR(50) NOT NULL,
            RESPONDENT_NAME VARCHAR(255),
            DATA_TYPE VARCHAR(20) NOT NULL,
            DATA_TYPE_NAME VARCHAR(255),
            VALUE FLOAT,
            VALUE_UNITS VARCHAR(50)
        )
        """)
        
        # Parallel fetch all regions
        all_records = []
        try:
            with ThreadPoolExecutor(max_workers=6) as executor:
                futures = {executor.submit(fetch_region_data, region, api_key): region for region in regions}
                
                for future in as_completed(futures):
                    region = futures[future]
                    try:
                        region_records = future.result()
                        all_records.extend(region_records)
                        logger.info(f"{region}: Added {len(region_records)} records to batch")
                    except Exception as e:
                        logger.error(f"{region}: Failed - {e}")
                        # Continue with other regions even if one fails
        except Exception as e:
            logger.error(f"Parallel fetch error: {e}")
            raise
        
        if not all_records:
            raise ValueError("No records fetched from any region")
        
        logger.info(f"Total records fetched: {len(all_records)}")
        
        # Transform and batch insert into temp table
        insert_sql = f"""
        INSERT INTO {DB}.{SCHEMA}.{TEMP_TABLE} 
        (PERIOD_DATE, RESPONDENT, RESPONDENT_NAME, DATA_TYPE, DATA_TYPE_NAME, VALUE, VALUE_UNITS)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        """
        
        batch = []
        total_inserted = 0
        
        for rec in all_records:
            try:
                # Transform timestamp
                period_str = rec.get('period', '')
                if 'T' in period_str:
                    date_part, hour_part = period_str.split('T')
                    timestamp = f"{date_part} {hour_part}:00:00"
                else:
                    timestamp = period_str
                
                # Prepare tuple (not list!)
                row = (
                    timestamp,
                    rec.get('respondent', ''),
                    rec.get('respondent-name', ''),
                    rec.get('type', ''),
                    rec.get('type-name', ''),
                    rec.get('value'),
                    rec.get('value-units', 'MWh')
                )
                
                batch.append(row)
                
                # Batch insert every BATCH_SIZE records
                if len(batch) >= BATCH_SIZE:
                    cur.executemany(insert_sql, batch)
                    total_inserted += len(batch)
                    logger.info(f"Inserted {total_inserted}/{len(all_records)} records into temp table")
                    batch = []
                    
            except Exception as e:
                logger.warning(f"Transform error for record: {e}")
                continue
        
        # Insert remaining records
        if batch:
            cur.executemany(insert_sql, batch)
            total_inserted += len(batch)
            logger.info(f"Inserted final batch: {total_inserted} total records")
        
        # Verify temp table
        cur.execute(f"SELECT COUNT(*) FROM {DB}.{SCHEMA}.{TEMP_TABLE}")
        temp_count = cur.fetchone()[0]
        logger.info(f"Temp table contains {temp_count} records")
        
        if temp_count == 0:
            raise ValueError("Temp table is empty after insert")
        
        # COMMIT transaction
        cur.execute("COMMIT")
        logger.info(f"✅ ETL fetch complete: {temp_count} records loaded to temp table")
        
        # Push only the count to XCom (not the data!)
        context['task_instance'].xcom_push(key='records_loaded', value=temp_count)
        
        return temp_count
        
    except Exception as e:
        # ROLLBACK on error
        logger.error(f"❌ Fetch and load error: {e}")
        try:
            cur.execute("ROLLBACK")
            logger.info("Transaction rolled back")
        except Exception as rollback_error:
            logger.error(f"Rollback failed: {rollback_error}")
        raise
    finally:
        cur.close()



# ========================
# Task 2: Merge from Temp to Main Table
# ========================
def merge_to_main_table(**context):
    """
    Perform MERGE (UPSERT) from temp table to main table (INCREMENTAL)
    - Updates existing records, inserts new ones
    - BEGIN/COMMIT/ROLLBACK transaction
    - Try/except/raise error handling
    - Idempotent: Can re-run same data without duplicates
    - Backfill support: Works with historical data
    """
    execution_date = context.get('ds', 'manual')
    
    try:
        records_loaded = context['task_instance'].xcom_pull(
            key='records_loaded', 
            task_ids='fetch_and_load_to_temp'
        )
    except Exception as e:
        logger.error(f"Failed to get XCom data: {e}")
        raise
    
    if not records_loaded or records_loaded == 0:
        logger.warning("No records to merge")
        return 0
    
    logger.info(f"Starting MERGE of {records_loaded} records for execution_date: {execution_date}")
    
    cur = get_snowflake_cursor()
    
    try:
        # BEGIN transaction
        logger.info("Starting MERGE transaction...")
        cur.execute("BEGIN")
        
        # Verify temp table exists and has data
        cur.execute(f"SELECT COUNT(*) FROM {DB}.{SCHEMA}.{TEMP_TABLE}")
        temp_count = cur.fetchone()[0]
        logger.info(f"Temp table has {temp_count} records to merge")
        
        if temp_count == 0:
            raise ValueError("Temp table is empty - nothing to merge")
        
        # Perform MERGE (UPSERT) - Idempotent operation
        merge_sql = f"""
        MERGE INTO {DB}.{SCHEMA}.{TABLE} AS target
        USING {DB}.{SCHEMA}.{TEMP_TABLE} AS source
        ON target.PERIOD_DATE = source.PERIOD_DATE
           AND target.RESPONDENT = source.RESPONDENT
           AND target.DATA_TYPE = source.DATA_TYPE
        WHEN MATCHED THEN
            UPDATE SET
                target.RESPONDENT_NAME = source.RESPONDENT_NAME,
                target.DATA_TYPE_NAME = source.DATA_TYPE_NAME,
                target.VALUE = source.VALUE,
                target.VALUE_UNITS = source.VALUE_UNITS,
                target.LOADED_AT = CURRENT_TIMESTAMP()
        WHEN NOT MATCHED THEN
            INSERT (PERIOD_DATE, RESPONDENT, RESPONDENT_NAME, DATA_TYPE, DATA_TYPE_NAME, VALUE, VALUE_UNITS)
            VALUES (source.PERIOD_DATE, source.RESPONDENT, source.RESPONDENT_NAME, 
                    source.DATA_TYPE, source.DATA_TYPE_NAME, source.VALUE, source.VALUE_UNITS)
        """
        
        logger.info("Executing MERGE (UPSERT)...")
        cur.execute(merge_sql)
        
        # Get merge statistics
        cur.execute(f"SELECT COUNT(*) FROM {DB}.{SCHEMA}.{TABLE}")
        total_count = cur.fetchone()[0]
        logger.info(f"Main table now has {total_count} total records")
        
        # Get records for this execution date (for verification)
        cur.execute(f"""
        SELECT COUNT(*) FROM {DB}.{SCHEMA}.{TABLE}
        WHERE DATE(LOADED_AT) = CURRENT_DATE()
        """)
        today_count = cur.fetchone()[0]
        logger.info(f"Records loaded today: {today_count}")
        
        # Clean up temp table
        cur.execute(f"DROP TABLE IF EXISTS {DB}.{SCHEMA}.{TEMP_TABLE}")
        logger.info("Temp table dropped")
        
        # COMMIT transaction
        cur.execute("COMMIT")
        logger.info(f"✅ MERGE complete: {total_count} total records in main table")
        
        return total_count
        
    except Exception as e:
        # ROLLBACK on error
        logger.error(f"❌ MERGE error: {e}")
        try:
            cur.execute("ROLLBACK")
            logger.info("Transaction rolled back")
        except Exception as rollback_error:
            logger.error(f"Rollback failed: {rollback_error}")
        raise
    finally:
        cur.close()



# ========================
# DAG Tasks
# ========================

fetch_and_load_task = PythonOperator(
    task_id='fetch_and_load_to_temp',
    python_callable=fetch_and_load_to_temp,
    dag=dag,
)

merge_task = PythonOperator(
    task_id='merge_to_main_table',
    python_callable=merge_to_main_table,
    dag=dag,
)

# Trigger dbt DAG after ETL completes
trigger_dbt = TriggerDagRunOperator(
    task_id='trigger_dbt_transform',
    trigger_dag_id='electricity_dbt_transform',
    wait_for_completion=False,  # Don't wait, let it run independently
    dag=dag,
)

# ========================
# Task Dependencies
# ========================

fetch_and_load_task >> merge_task >> trigger_dbt